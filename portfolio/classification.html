<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
<head>
  <title>Classification</title>
</head>
<body>
<h2>Introduction</h2>
<p>Unlike the unsupervised learning that was covered in the previous section on clustering, classification is a supervised algorithm that attempts to classify new data according to previous known classes. For example, take specieis of animals. The classes (except in rare cases) for species are already known. An animal is either an amphibian, a reptile, a fish, a bird, or a mammal. There are certain characteristics that make an animal a particular species. Using data about animal characteristics for which the class is already known, a model can be developed for correctly classifying an animal based on these characteristics. Then, when a new animal is discovered, it too can be correctly classified according to it's characteristics. This is a supervised system because the class labels are all known in advance, and the goal is to apply the correct class label to the newly discovered animal. A few different algorithms cas be used to develop this classification model. This section will cover a few of these classifiers, including decision trees, rule-based classifiers, bayesian classifiers, k-nearest neighbor, and artificial neural networks. After the discussion of algorithms, a Ruby implementation of an artificial neural network is presented and discussed.</p>
<h2>Decision Tree Classification</h2>
<img src="images/d-tree.png" />
<p>A decision tree is just that, a tree used to make decisions. Each node on the tree asks a question. This is most commonly a binary question, as most decisions trees are binary, meaning each node has at most two child nodes. For example, take a look at the tree above. This might be a tree used in the animal classification problem. The first node asks "Is the animal warm blodded?". If so, then the algorithm would continue down the left branch of the tree, and the next question might be "Does the animal give birth?". A data object would continue down the tree in such a fashion until it landed in a leaf node. Leaf nodes of the tree contain the classification of the object. In other words, when the object traverses a tree and gets to a leaf node, that node will define it's classification. For example, if an animal is warm blooded and it gives birth, then it is a mammal, and the classification is complete. Since decision trees need to ask questions with discrete answers, data may need to be pre-processed and descretized. For example, if the problem is to classify whether or not someone is a good candidate for a mortgage, then things like house-hold income and credit score might be taken into account. Since these are continuous attributes, they may need to be descretized into bins. For example, a node might split based on the condition of "is income > $45,000 a year?", or "is credit score less than 600?".</p>
<p>A decision tree can be built using Hunt's algorithm, which is a recursive process. At each node, if all the records from the training data in that node share the same class, then the algorithm is complete, and the node is left as a leaf node. If the node contains records that belong to more than one class, then the records need to be dvided according to an attribute test condition. This process is repeated until all items are in leaf node with fellow class-members, or some other stopping condition is met. Attribute test conditions are often binary splits, though multiway splits are also possible. The hardest part about building the tree is selecting the best attribute test condition, in other words, the best split. There are three common impurity measures used to measure the best split. Since the goal of a decision tree is to have nodes consisting entirely of memebers of a single class, the impurity of a node is the extent to which that is not the case. For example, a node with 2 members of one class, and 0 members of another class has zero impurity. A node with 1 member of one class, and one of another, however, has the highest impurity. The three most common measures of impurity are entropy, gini impurity, and classification error. They are definied using the following equations, where <em>p(i|t)</em> denotes the fraction of records that belong to class <em>i</em> at a given node <em>t</em>, and <em>c</em> is the number of classes:</p>
<img src="images/impurity.png" />
<p>The performance of a test condition is determined by comparing the degree of impurity in the parent node to that of the child node. The larger the difference, the more pure the child nodes are, and the better the test condition. The measure is called simply gain, and is denoted with a delta symbol. When entropy is used as the impurity, this measure is referred to as information gain. The equation for information gain is:</p>
<img src="images/gain.png" />
<p>Where I(*) is the impurity measure of a given node, N is the total number or records at the given node's parent, k is the numer of attribute values, and N(v<sub>j</sub>) is te number of records associated with the child node, v<sub>j</sub>.</p>
<p>As mentioned earlier, the process of building a decision tree continues until some stopping condition is met. This stopping condition could be when al lthe leaf nodes are pure, or when the number of records fall below a certain threshold. For example, when 90% of the records have been classifed, it may be a good time to stop building the tree. If the tree gets too large, it could succumb to phenomenon of model overfitting. This is when the model is overly trained to the training data, so that while it does a great job of classifying the training data, it is only good at classifying that training data, and fails to successfully classify new data. In order to prevent model over-fitting, decision trees can be pruned after the are build. There are a few different ways to prune the tree, but the overall goal is to improve it's generalization capabilities by reducing the size of unecessarily long or wide branches. Decision trees are best used in situations where knowledge of how the model is doing the classification is useful. Since the attribute test at each level can be seen, a decision tree can give a user insight into how best approach and walk through the classification process. This could be considered a "white box" model, in contrast to the "black box" of artificial neural networks, which will be discussed later. Accurate decision trees can also be created using very few training records, which is another benefit of this method.</p>
<h2>Rule-Based Classification</h2>



</body>
</html>