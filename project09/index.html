<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>
<head>
  <title>Project 9 - YDD Cup</title>
</head>
<body>

<p>My first step in building an information platform to mine this data was to get everything into a relational database. This would be the foundation of my platform, providing easy access to scripts and other tools I would use to look for patterns. A relational database made the data easier to browse, as well as making it easier to generate summary statistics and write scripts to both extract and create features if necessary. Due to the size of the data, however, this step was more difficult, or at least time consuming, than it should have been. It was more time consuming because I was sloppy, and did not develop in a test-driven manner. I should have exctracted a few records from each of the data files that had relations to one another, and tested my database populating algorithm with that. By making sure I had records representing all variables I would encounter, such as albums or tracks without artists, tracks without albums, and albums and tracks both with and without genres, I could verify my algorithm using a much smaller dataset. Since I did not do this, and instead ran each iteration on the entire dataset, every error took exponentially longer to discover. For example, I had a bug that nested the <code>album.save</code> command inside the conditional block that only ran if the element had genres associated with it. Thus, all albums without genres were not getting saved. This was hard to detect, however, since most of the records were still being saved, so the script was processing a lot of albums, making it look like everything was working fine. I did not detect the problem until the algorithm moved on to adding tracks. It got about 15 tracks in until it spit up an error that the specified album ID for the currect track could not be found. It was not until this point that I took a count of the database records in the Albums table and discovered I was missing a bunch of records. Also, because there still were tens of thousands or records in there, it took a while to pin-point the missing record I was looking for, so that I could see which records were saved before and after it in order to figure out my problem. I've included this antectdote because it was important realization about builing and information platform. When working with large datasets, it is essential to test import and processing scripts on a subset of the data. It takes a long time to work through large datasets, and this procedure is often done in a batch job, overnight, on another machine, or in parallel. Therefore, errors may not be caught until hours, or even days later, at which point the entire process may have to be restarted. Testing scripts on smaller subsets of the data gives immediate feedback, so that all problems can be fixed before the rest of the data is processed. This was a lesson I learned the hard way, and will keep in mind for future mining endeavors.</p>
<p></p>

</body>
</html>